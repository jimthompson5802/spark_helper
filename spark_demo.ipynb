{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07471a46-7f45-4cf9-8366-5f22d782b91d",
   "metadata": {},
   "source": [
    "## Run pip install, then restart kernel to pickup newly installed library before running rest of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c7dd3a4-8e86-4690-be27-48c84dbba777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/jovyan\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyspark>=3.0.0 in /usr/local/spark/python (from spark_helper==0.0.1.dev0) (4.0.0.dev2)\n",
      "Requirement already satisfied: pyyaml>=5.4.0 in /opt/conda/lib/python3.12/site-packages (from spark_helper==0.0.1.dev0) (6.0.2)\n",
      "Collecting py4j==0.10.9.7 (from pyspark>=3.0.0->spark_helper==0.0.1.dev0)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: spark_helper\n",
      "  Building editable for spark_helper (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for spark_helper: filename=spark_helper-0.0.1.dev0-0.editable-py3-none-any.whl size=5665 sha256=5f20cb0960b726732425e1293f71ae8f7dcc3973cf95da973a5ae7a685cfb4d4\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-elmzbccn/wheels/fb/60/5d/f5254a4bf81e0cdb197add98454cd863e4ecd55e30e419f272\n",
      "Successfully built spark_helper\n",
      "Installing collected packages: py4j, spark_helper\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [spark_helper]\n",
      "\u001b[1A\u001b[2KSuccessfully installed py4j-0.10.9.7 spark_helper-0.0.1.dev0\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ee9171-396a-4c62-b2db-72f5bc0e1ee5",
   "metadata": {},
   "source": [
    "Restart Kernel before continuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ebc3ccf-a192-44b4-b5f6-7517f8ae3d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_helper.core import create_spark_session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01aa919-30c3-4087-a5f4-6aab399ba920",
   "metadata": {},
   "source": [
    "## Test Spark cluster from Configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56456ba4-d66b-41bb-b6d5-baac51cdc0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session(\"sample_cluster_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "001b35b2-a0fc-44bd-8c7b-f79a8922b8c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize(range(10))\n",
    "\n",
    "rdd.map(lambda x: x*x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0a8a39b-2440-44b5-aa92-d834c122b939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  spark.app.id: app-20250518211310-0000\n",
      "  spark.app.name: SparkHelperApp\n",
      "  spark.app.startTime: 1747602790031\n",
      "  spark.app.submitTime: 1747602789941\n",
      "  spark.broadcast.compress: true\n",
      "  spark.driver.cores: 2\n",
      "  spark.driver.extraJavaOptions: -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true\n",
      "  spark.driver.host: cd5619462bbc\n",
      "  spark.driver.maxResultSize: 1g\n",
      "  spark.driver.memory: 4g\n",
      "  spark.driver.port: 38603\n",
      "  spark.driver.pythonVersion: 3.11\n",
      "  spark.executor.cores: 2\n",
      "  spark.executor.extraJavaOptions: -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true\n",
      "  spark.executor.heartbeatInterval: 60s\n",
      "  spark.executor.id: driver\n",
      "  spark.executor.instances: 2\n",
      "  spark.executor.memory: 4g\n",
      "  spark.executor.pythonVersion: 3.11\n",
      "  spark.hadoop.fs.s3a.connection.establish.timeout: 30000\n",
      "  spark.master: spark://spark-master:7077\n",
      "  spark.network.timeout: 300s\n",
      "  spark.rdd.compress: True\n",
      "  spark.serializer.objectStreamReset: 100\n",
      "  spark.sql.adaptive.enabled: true\n",
      "  spark.submit.deployMode: client\n",
      "  spark.submit.pyFiles: \n",
      "  spark.ui.showConsoleProgress: true\n"
     ]
    }
   ],
   "source": [
    "for item in sorted(spark.sparkContext.getConf().getAll()):\n",
    "    print(f\"  {item[0]}: {item[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e411f803-4b16-4b81-9553-91c1fb2833ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222ef6e3-1bba-4777-ad18-9f51a2ee73dd",
   "metadata": {},
   "source": [
    "## Test Spark cluster from Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9832131-86b5-42e8-8b1a-1b280ef9f1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"type\": \"cluster\",\n",
    "    \"appName\": \"SparkClusterAppFromDictionary\",\n",
    "    \"spark.executor.memory\": \"1g\",\n",
    "    \"spark.driver.memory\": \"1g\",\n",
    "    \"spark.driver.maxResultSize\": \"2g\",\n",
    "}\n",
    "spark = create_spark_session(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff3e8f6c-f551-45da-9d25-2d58c5a88558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize(range(10))\n",
    "\n",
    "rdd.map(lambda x: x*x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e98d20af-5097-41c7-b130-4bc346ef62dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  spark.app.id: app-20250518211317-0001\n",
      "  spark.app.name: SparkClusterAppFromDictionary\n",
      "  spark.app.startTime: 1747602797582\n",
      "  spark.app.submitTime: 1747602789941\n",
      "  spark.broadcast.compress: true\n",
      "  spark.driver.cores: 2\n",
      "  spark.driver.extraJavaOptions: -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true\n",
      "  spark.driver.host: cd5619462bbc\n",
      "  spark.driver.maxResultSize: 2g\n",
      "  spark.driver.memory: 1g\n",
      "  spark.driver.port: 46307\n",
      "  spark.driver.pythonVersion: 3.11\n",
      "  spark.executor.cores: 2\n",
      "  spark.executor.extraJavaOptions: -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true\n",
      "  spark.executor.heartbeatInterval: 60s\n",
      "  spark.executor.id: driver\n",
      "  spark.executor.instances: 2\n",
      "  spark.executor.memory: 1g\n",
      "  spark.executor.pythonVersion: 3.11\n",
      "  spark.hadoop.fs.s3a.connection.establish.timeout: 30000\n",
      "  spark.master: spark://spark-master:7077\n",
      "  spark.network.timeout: 300s\n",
      "  spark.rdd.compress: True\n",
      "  spark.serializer.objectStreamReset: 100\n",
      "  spark.sql.adaptive.enabled: true\n",
      "  spark.submit.deployMode: client\n",
      "  spark.submit.pyFiles: \n",
      "  spark.ui.showConsoleProgress: true\n"
     ]
    }
   ],
   "source": [
    "for item in sorted(spark.sparkContext.getConf().getAll()):\n",
    "    print(f\"  {item[0]}: {item[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54b9dbe7-453c-4c0f-9d57-0c999059fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb992ecc-0926-450f-9a0e-fb657deebb39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
